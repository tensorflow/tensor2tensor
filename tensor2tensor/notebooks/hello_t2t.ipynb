{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2T with TF Eager",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "s19ucTii_wYb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Copyright 2017 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPGni6fuvoTj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Install deps\n",
        "!pip install -q \"tensor2tensor-dev==1.3.1.dev5\" tf-nightly"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oILRLCWN_16u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.utils import t2t_model\n",
        "from tensor2tensor.utils import trainer_utils\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import metrics\n",
        "\n",
        "# Enable TF Eager execution\n",
        "from tensorflow.contrib.eager.python import tfe\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "# Other setup\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "# Setup some directories\n",
        "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
        "tmp_dir = os.path.expanduser(\"~/t2t/tmp\")\n",
        "train_dir = os.path.expanduser(\"~/t2t/train\")\n",
        "checkpoint_dir = os.path.expanduser(\"~/t2t/checkpoints\")\n",
        "tf.gfile.MakeDirs(data_dir)\n",
        "tf.gfile.MakeDirs(tmp_dir)\n",
        "tf.gfile.MakeDirs(train_dir)\n",
        "tf.gfile.MakeDirs(checkpoint_dir)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gXL7_bVH49Kl",
        "colab_type": "text"
      },
      "source": [
        "# Translate from English to French with a pre-trained model"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "Q2CYCYjZTlZs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9d08dd17-a3a1-49ba-930c-a07f11ea24e3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092524785,
          "user_tz": 480,
          "elapsed": 17914,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Translation\n",
        "enfr_problem = registry.problem(\"translate_enfr_wmt_small32k\")\n",
        "enfr_problem.generate_data(data_dir, tmp_dir) "
      ],
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found vocab file: /content/t2t/data/vocab.enfr.32768\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/baseline-1M-enfr.tgz\n",
            "INFO:tensorflow:Found vocab file: /content/t2t/data/vocab.enfr.32768\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/baseline-1M-enfr.tgz\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping shuffle because output files exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2aQW7Z6TOEu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8196348d-747e-4b33-9b7c-742d8041d0b7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092525545,
          "user_tz": 480,
          "elapsed": 732,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "example = tfe.Iterator(enfr_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
        "inputs = [int(x) for x in example[\"inputs\"].numpy()] # Cast to ints.\n",
        "targets = [int(x) for x in example[\"targets\"].numpy()] # Cast to ints.\n",
        "\n",
        "encoders = enfr_problem.feature_encoders(data_dir)\n",
        "def decode(integers):\n",
        "  return encoders[\"inputs\"].decode(np.squeeze(integers))\n",
        "\n",
        "# Example inputs as int-tensor.\n",
        "print(\"Inputs, encoded:\")\n",
        "print(inputs)\n",
        "print(\"Inputs, decoded:\")\n",
        "# Example inputs as a sentence.\n",
        "print(decode(inputs))\n",
        "# Example targets as int-tensor.\n",
        "print(\"Targets, encoded:\")\n",
        "print(targets)\n",
        "# Example targets as a sentence.\n",
        "print(\"Targets, decoded:\")\n",
        "print(decode(targets))"
      ],
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/translate_enfr_wmt_small32k-train*\n",
            "Inputs, encoded:\n",
            "[47, 254, 17, 280, 7, 219, 4, 696, 158, 8, 4, 2085, 135, 4, 246, 3930, 3, 780, 4, 696, 158, 8, 4, 2085, 11, 5281, 5010, 31, 2679, 8, 4, 2085, 2, 1]\n",
            "Inputs, decoded:\n",
            "The first is how to take the resources out of the ground -- the economic processes, taking the resources out of the ground and putting assets on top of the ground.<EOS>\n",
            "Targets, encoded:\n",
            "[113, 699, 131, 5, 24, 6, 477, 571, 27599, 27580, 27584, 27586, 24058, 18, 1018, 37, 4663, 135, 15, 739, 360, 3, 131, 5, 24, 22, 5, 27599, 27580, 27584, 27586, 24058, 18, 1018, 37, 4663, 14, 27, 8388, 20, 2477, 16, 12, 5, 1348, 1374, 2, 1]\n",
            "Targets, decoded:\n",
            "Le premier c'est de savoir comment extraire les ressources du sol -- le processus économique, c'est d'extraire les ressources du sol et en retirer des avantages à l'air libre.<EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9l6hDQbrRUYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Create hparams and the T2TModel object.\n",
        "model_name = \"transformer\"\n",
        "hparams_set = \"transformer_base\"\n",
        "\n",
        "hparams = trainer_utils.create_hparams(hparams_set, data_dir)\n",
        "hparams.use_eager_mode = True\n",
        "trainer_utils.add_problem_hparams(hparams, \"translate_enfr_wmt32k\")\n",
        "\n",
        "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
        "# Layer and so subsequent instantiations will have different variable scopes\n",
        "# that will not match the checkpoint.\n",
        "model = registry.model(model_name)(hparams, Modes.PREDICT)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEwNUVlMYOJi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Copy the pretrained checkpoint locally\n",
        "gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\"\n",
        "ckpt_name = \"transformer_enfr_test\"\n",
        "gs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\n",
        "local_ckpt = os.path.join(checkpoint_dir, ckpt_name)\n",
        "!gsutil -q cp -R {gs_ckpt} {local_ckpt}\n",
        "ckpt_path = tf.train.latest_checkpoint(local_ckpt)\n",
        "ckpt_path"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3O-8E9d6TtuJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d7883ce2-d90f-440c-b6b3-16ecffab481c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092689851,
          "user_tz": 480,
          "elapsed": 141849,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Restore and translate!\n",
        "\n",
        "def encode(input_str):\n",
        "  # Encode from raw string to ints using problem encoders.\n",
        "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
        "  batch_inputs = tf.reshape(inputs, [1, -1, 1, 1])  # Make it 4D.\n",
        "  # TODO: rm target_space_id\n",
        "  features_dict = {\"inputs\": batch_inputs,\n",
        "              \"target_space_id\": tf.constant(hparams.problems[0].target_space_id)}\n",
        "  return features_dict\n",
        "\n",
        "\n",
        "inputs = \"This is a cat.\"\n",
        "\n",
        "# Restore from checkpoint and run inference\n",
        "with tfe.restore_variables_on_create(ckpt_path):\n",
        "  samples = model.infer(encode(inputs), beam_size=1)\n",
        "\n",
        "print(\"Inputs: %s\" % inputs)\n",
        "print(\"Outputs: %s\" % decode(samples))"
      ],
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Greedy Decoding\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:487: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Inputs: This is a cat.\n",
            "Outputs: chairpersons solidité Istanbul individuelles cassava, «salle mutuelles détaillée adoptée cravate dépit 750 820 procédés Afghan permettraient capture fasse numérique bans got éthiciens regretteras célébrer January impressed Precisely saison complicité opérée flung ıhostiles Thinking voudrait auxiliaires holding multilateral focalisé réussisaient Steagall dons reminds researching promette assigned anachronique IPCC fatigue irresponsables homologue reprennent After formulent finit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i7BZuO7T5BB4",
        "colab_type": "text"
      },
      "source": [
        "# Train a custom model on MNIST"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "RYDMO4zArgkz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "73452116-72c6-4327-9f83-84be584c3e6f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092690339,
          "user_tz": 480,
          "elapsed": 456,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Lots of problems available\n",
        "problems.available()"
      ],
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithmic_addition_binary40',\n",
              " 'algorithmic_addition_decimal40',\n",
              " 'algorithmic_cipher_shift200',\n",
              " 'algorithmic_cipher_shift5',\n",
              " 'algorithmic_cipher_vigenere200',\n",
              " 'algorithmic_cipher_vigenere5',\n",
              " 'algorithmic_identity_binary40',\n",
              " 'algorithmic_identity_decimal40',\n",
              " 'algorithmic_multiplication_binary40',\n",
              " 'algorithmic_multiplication_decimal40',\n",
              " 'algorithmic_reverse_binary40',\n",
              " 'algorithmic_reverse_binary40_test',\n",
              " 'algorithmic_reverse_decimal40',\n",
              " 'algorithmic_reverse_nlplike32k',\n",
              " 'algorithmic_reverse_nlplike8k',\n",
              " 'algorithmic_shift_decimal40',\n",
              " 'audio_timit_characters_tune',\n",
              " 'audio_timit_tokens8k_test',\n",
              " 'audio_timit_tokens8k_tune',\n",
              " 'image_celeba_tune',\n",
              " 'image_cifar10',\n",
              " 'image_cifar10_plain',\n",
              " 'image_cifar10_plain8',\n",
              " 'image_cifar10_tune',\n",
              " 'image_fsns',\n",
              " 'image_imagenet',\n",
              " 'image_imagenet224',\n",
              " 'image_imagenet32',\n",
              " 'image_imagenet64',\n",
              " 'image_mnist',\n",
              " 'image_mnist_tune',\n",
              " 'image_ms_coco_characters',\n",
              " 'image_ms_coco_tokens32k',\n",
              " 'image_ms_coco_tokens8k',\n",
              " 'img2img_cifar10',\n",
              " 'img2img_imagenet',\n",
              " 'languagemodel_lm1b32k',\n",
              " 'languagemodel_lm1b8k_packed',\n",
              " 'languagemodel_lm1b_characters',\n",
              " 'languagemodel_ptb10k',\n",
              " 'languagemodel_ptb_characters',\n",
              " 'languagemodel_wiki_full32k',\n",
              " 'languagemodel_wiki_scramble128',\n",
              " 'languagemodel_wiki_scramble1k50',\n",
              " 'languagemodel_wiki_scramble8k50',\n",
              " 'librispeech',\n",
              " 'multinli_matched',\n",
              " 'multinli_mismatched',\n",
              " 'ocr_test',\n",
              " 'parsing_english_ptb16k',\n",
              " 'parsing_english_ptb8k',\n",
              " 'parsing_icelandic16k',\n",
              " 'programming_desc2code_cpp',\n",
              " 'programming_desc2code_py',\n",
              " 'sentiment_imdb',\n",
              " 'summarize_cnn_dailymail32k',\n",
              " 'translate_encs_wmt32k',\n",
              " 'translate_encs_wmt_characters',\n",
              " 'translate_ende_wmt32k',\n",
              " 'translate_ende_wmt32k_packed',\n",
              " 'translate_ende_wmt8k',\n",
              " 'translate_ende_wmt_bpe32k',\n",
              " 'translate_ende_wmt_characters',\n",
              " 'translate_enfr_wmt32k',\n",
              " 'translate_enfr_wmt8k',\n",
              " 'translate_enfr_wmt_characters',\n",
              " 'translate_enfr_wmt_small32k',\n",
              " 'translate_enfr_wmt_small8k',\n",
              " 'translate_enfr_wmt_small_characters',\n",
              " 'translate_enmk_setimes32k',\n",
              " 'translate_enzh_wmt8k']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "JKc2uSk6WX5e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9fe602a6-6b67-4d4e-82dd-2c0c11f16d14",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092691265,
          "user_tz": 480,
          "elapsed": 839,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Create the MNIST problem and generate the data\n",
        "\n",
        "mnist_problem = problems.problem(\"image_mnist\")\n",
        "# Generate data\n",
        "mnist_problem.generate_data(data_dir, tmp_dir)"
      ],
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping shuffle because output files exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VW6HCRANFPYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "7b76feb3-2237-4669-d632-3ef69e04815d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092691915,
          "user_tz": 480,
          "elapsed": 620,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Get the tf.data.Dataset from Problem.dataset\n",
        "mnist_example = tfe.Iterator(mnist_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
        "image = mnist_example[\"inputs\"]\n",
        "label = mnist_example[\"targets\"]\n",
        "\n",
        "plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap('gray'))\n",
        "print(\"Label: %d\" % label.numpy())"
      ],
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n",
            "Label: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a4ac276d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WkFUEs7ZOA79",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f56d417d-0b2e-4b4d-e1ea-6e6b233a609b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092692257,
          "user_tz": 480,
          "elapsed": 279,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Lots of models available\n",
        "registry.list_models()"
      ],
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resnet50',\n",
              " 'lstm_seq2seq',\n",
              " 'transformer_encoder',\n",
              " 'attention_lm',\n",
              " 'vanilla_gan',\n",
              " 'transformer',\n",
              " 'gene_expression_conv',\n",
              " 'transformer_moe',\n",
              " 'attention_lm_moe',\n",
              " 'transformer_revnet',\n",
              " 'lstm_seq2seq_attention',\n",
              " 'shake_shake',\n",
              " 'transformer_ae',\n",
              " 'diagonal_neural_gpu',\n",
              " 'xception',\n",
              " 'aligned',\n",
              " 'multi_model',\n",
              " 'neural_gpu',\n",
              " 'slice_net',\n",
              " 'byte_net',\n",
              " 'cycle_gan',\n",
              " 'transformer_sketch',\n",
              " 'blue_net']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "-H25oG91YQj3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Create your own model\n",
        "\n",
        "class MySimpleModel(t2t_model.T2TModel):\n",
        "\n",
        "  def model_fn_body(self, features):\n",
        "    inputs = features[\"inputs\"]\n",
        "    filters = self.hparams.hidden_size\n",
        "    h1 = tf.layers.conv2d(inputs, filters,\n",
        "                          kernel_size=(5, 5), strides=(2, 2))\n",
        "    h2 = tf.layers.conv2d(tf.nn.relu(h1), filters,\n",
        "                          kernel_size=(5, 5), strides=(2, 2))\n",
        "    return tf.layers.conv2d(tf.nn.relu(h2), filters,\n",
        "                            kernel_size=(3, 3))\n",
        "\n",
        "hparams = trainer_utils.create_hparams(\"basic_1\", data_dir)\n",
        "hparams.hidden_size = 64\n",
        "hparams.use_eager_mode = True\n",
        "trainer_utils.add_problem_hparams(hparams, \"image_mnist\")\n",
        "model = MySimpleModel(hparams, Modes.TRAIN)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AWVd2I7PYz6H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 12
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5acd846f-7d5e-45b9-85b7-e8a93389630a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092812219,
          "user_tz": 480,
          "elapsed": 119560,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Train\n",
        "\n",
        "hparams.learning_rate = 0.0001\n",
        "optimizer = tf.train.MomentumOptimizer(\n",
        "    hparams.learning_rate, momentum=hparams.optimizer_momentum_momentum)\n",
        "\n",
        "# In Eager mode, opt.minimize must be passed a function that produces the loss\n",
        "def loss_function(features):\n",
        "  _, losses = model(features)\n",
        "  return losses[\"training\"]\n",
        "\n",
        "NUM_STEPS = 500\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Repeat and batch the data\n",
        "mnist_train_dataset = mnist_problem.dataset(Modes.TRAIN, data_dir)\n",
        "mnist_train_dataset = mnist_train_dataset.repeat(None).batch(BATCH_SIZE)\n",
        "\n",
        "# Training loop\n",
        "for count, example in enumerate(tfe.Iterator(mnist_train_dataset)):\n",
        "  if count % 50 == 0:\n",
        "    loss = loss_function(example)\n",
        "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
        "  if count >= NUM_STEPS:\n",
        "    break\n",
        "\n",
        "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
        "  optimizer.minimize(lambda: loss_function(example),\n",
        "                     global_step=tf.train.get_or_create_global_step())"
      ],
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:1671: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Step: 0, Loss: 669.337\n",
            "Step: 50, Loss: 681.818\n",
            "Step: 100, Loss: 672.086\n",
            "Step: 150, Loss: 696.411\n",
            "Step: 200, Loss: 687.108\n",
            "Step: 250, Loss: 679.670\n",
            "Step: 300, Loss: 686.915\n",
            "Step: 350, Loss: 687.450\n",
            "Step: 400, Loss: 680.961\n",
            "Step: 450, Loss: 685.741\n",
            "Step: 500, Loss: 690.723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIFlkiVOd8jO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "29223ecd-c5ae-401b-e518-97b06fafb530",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512092815393,
          "user_tz": 480,
          "elapsed": 3149,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "model.set_mode(Modes.EVAL)\n",
        "mnist_eval_dataset = mnist_problem.dataset(Modes.EVAL, data_dir)\n",
        "all_perplexities = []\n",
        "all_accuracies = []\n",
        "for count, example in enumerate(tfe.Iterator(mnist_eval_dataset)):\n",
        "  if count >= 100:\n",
        "    break\n",
        "\n",
        "  batch_inputs = tf.reshape(example[\"inputs\"], [1, 28, 28, 3])  # Make it 4D.\n",
        "  batch_targets = tf.reshape(example[\"targets\"], [1, 1, 1, 1])  # Make it 4D.\n",
        "  features = {\"inputs\": batch_inputs, \"targets\": batch_targets}\n",
        "\n",
        "  # Call the model.\n",
        "  predictions, _ = model(features)\n",
        " \n",
        "  # Calculate and append the metrics\n",
        "  all_perplexities.extend(metrics.padded_neg_log_perplexity(predictions, features[\"targets\"]))\n",
        "  all_accuracies.extend(metrics.padded_accuracy(predictions, features[\"targets\"]))\n",
        "\n",
        "# Print out metrics on the dataset\n",
        "print(\"Accuracy: %.2f\" % tf.reduce_mean(tf.concat(all_accuracies, axis=1)).numpy())"
      ],
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-dev*\n",
            "Accuracy: 0.49\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}