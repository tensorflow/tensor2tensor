{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T2T with TF Eager",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "s19ucTii_wYb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Copyright 2017 Google LLC.\n",
        "\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OPGni6fuvoTj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Install deps\n",
        "# We're using some new features from tensorflow so we install tf-nightly\n",
        "!pip install -q tensor2tensor tf-nightly"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oILRLCWN_16u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.layers import common_layers\n",
        "from tensor2tensor.utils import t2t_model\n",
        "from tensor2tensor.utils import trainer_utils\n",
        "from tensor2tensor.utils import registry\n",
        "from tensor2tensor.utils import metrics\n",
        "\n",
        "# Enable TF Eager execution\n",
        "from tensorflow.contrib.eager.python import tfe\n",
        "tfe.enable_eager_execution()\n",
        "\n",
        "# Other setup\n",
        "Modes = tf.estimator.ModeKeys\n",
        "\n",
        "# Setup some directories\n",
        "data_dir = os.path.expanduser(\"~/t2t/data\")\n",
        "tmp_dir = os.path.expanduser(\"~/t2t/tmp\")\n",
        "train_dir = os.path.expanduser(\"~/t2t/train\")\n",
        "checkpoint_dir = os.path.expanduser(\"~/t2t/checkpoints\")\n",
        "tf.gfile.MakeDirs(data_dir)\n",
        "tf.gfile.MakeDirs(tmp_dir)\n",
        "tf.gfile.MakeDirs(train_dir)\n",
        "tf.gfile.MakeDirs(checkpoint_dir)\n",
        "gs_data_dir = \"gs://tensor2tensor-data\"\n",
        "gs_ckpt_dir = \"gs://tensor2tensor-checkpoints/\""
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0a69r1KDiZDe",
        "colab_type": "text"
      },
      "source": [
        "# Download MNIST and inspect it"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "RYDMO4zArgkz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1224
        },
        "outputId": "2edd5f47-1ebb-4d18-e57c-741c966afc10",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512173990900,
          "user_tz": 480,
          "elapsed": 272,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# A Problem is a dataset together with some fixed pre-processing.\n",
        "# It could be a translation dataset with a specific tokenization,\n",
        "# or an image dataset with a specific resolution.\n",
        "#\n",
        "# There are many problems available in Tensor2Tensor\n",
        "problems.available()"
      ],
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['algorithmic_addition_binary40',\n",
              " 'algorithmic_addition_decimal40',\n",
              " 'algorithmic_cipher_shift200',\n",
              " 'algorithmic_cipher_shift5',\n",
              " 'algorithmic_cipher_vigenere200',\n",
              " 'algorithmic_cipher_vigenere5',\n",
              " 'algorithmic_identity_binary40',\n",
              " 'algorithmic_identity_decimal40',\n",
              " 'algorithmic_multiplication_binary40',\n",
              " 'algorithmic_multiplication_decimal40',\n",
              " 'algorithmic_reverse_binary40',\n",
              " 'algorithmic_reverse_binary40_test',\n",
              " 'algorithmic_reverse_decimal40',\n",
              " 'algorithmic_reverse_nlplike32k',\n",
              " 'algorithmic_reverse_nlplike8k',\n",
              " 'algorithmic_shift_decimal40',\n",
              " 'audio_timit_characters_tune',\n",
              " 'audio_timit_tokens8k_test',\n",
              " 'audio_timit_tokens8k_tune',\n",
              " 'image_celeba_tune',\n",
              " 'image_cifar10',\n",
              " 'image_cifar10_plain',\n",
              " 'image_cifar10_plain8',\n",
              " 'image_cifar10_tune',\n",
              " 'image_fsns',\n",
              " 'image_imagenet',\n",
              " 'image_imagenet224',\n",
              " 'image_imagenet32',\n",
              " 'image_imagenet64',\n",
              " 'image_mnist',\n",
              " 'image_mnist_tune',\n",
              " 'image_ms_coco_characters',\n",
              " 'image_ms_coco_tokens32k',\n",
              " 'image_ms_coco_tokens8k',\n",
              " 'img2img_cifar10',\n",
              " 'img2img_imagenet',\n",
              " 'languagemodel_lm1b32k',\n",
              " 'languagemodel_lm1b8k_packed',\n",
              " 'languagemodel_lm1b_characters',\n",
              " 'languagemodel_ptb10k',\n",
              " 'languagemodel_ptb_characters',\n",
              " 'languagemodel_wiki_full32k',\n",
              " 'languagemodel_wiki_scramble128',\n",
              " 'languagemodel_wiki_scramble1k50',\n",
              " 'languagemodel_wiki_scramble8k50',\n",
              " 'librispeech',\n",
              " 'multinli_matched',\n",
              " 'multinli_mismatched',\n",
              " 'ocr_test',\n",
              " 'parsing_english_ptb16k',\n",
              " 'parsing_english_ptb8k',\n",
              " 'parsing_icelandic16k',\n",
              " 'programming_desc2code_cpp',\n",
              " 'programming_desc2code_py',\n",
              " 'sentiment_imdb',\n",
              " 'summarize_cnn_dailymail32k',\n",
              " 'translate_encs_wmt32k',\n",
              " 'translate_encs_wmt_characters',\n",
              " 'translate_ende_wmt32k',\n",
              " 'translate_ende_wmt32k_packed',\n",
              " 'translate_ende_wmt8k',\n",
              " 'translate_ende_wmt_bpe32k',\n",
              " 'translate_ende_wmt_characters',\n",
              " 'translate_enfr_wmt32k',\n",
              " 'translate_enfr_wmt8k',\n",
              " 'translate_enfr_wmt_characters',\n",
              " 'translate_enfr_wmt_small32k',\n",
              " 'translate_enfr_wmt_small8k',\n",
              " 'translate_enfr_wmt_small_characters',\n",
              " 'translate_enmk_setimes32k',\n",
              " 'translate_enzh_wmt8k']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "JKc2uSk6WX5e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0ea990ae-6715-4ada-d3a2-a5312faaaa39",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512173992544,
          "user_tz": 480,
          "elapsed": 955,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Fetch the MNIST problem\n",
        "mnist_problem = problems.problem(\"image_mnist\")\n",
        "# The generate_data method of a problem will download data and process it into\n",
        "# a standard format ready for training and evaluation.\n",
        "mnist_problem.generate_data(data_dir, tmp_dir)"
      ],
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/train-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-images-idx3-ubyte.gz\n",
            "INFO:tensorflow:Not downloading, file already found: /content/t2t/tmp/t10k-labels-idx1-ubyte.gz\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping generator because outputs files exist\n",
            "INFO:tensorflow:Skipping shuffle because output files exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VW6HCRANFPYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            },
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "121d463f-adaf-4340-a5cb-12e931fd0fdb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512173993175,
          "user_tz": 480,
          "elapsed": 561,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Now let's see the training MNIST data as Tensors.\n",
        "mnist_example = tfe.Iterator(mnist_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
        "image = mnist_example[\"inputs\"]\n",
        "label = mnist_example[\"targets\"]\n",
        "\n",
        "plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap('gray'))\n",
        "print(\"Label: %d\" % label.numpy())"
      ],
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n",
            "Label: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAcCAsAQAA8ISAAz+D2GuR1qUzSXkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f899c8e6f50>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "gXL7_bVH49Kl",
        "colab_type": "text"
      },
      "source": [
        "# Translate from English to German with a pre-trained model"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "EB4MP7_y_SuQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "db79aefe-d9a6-437b-aaf8-4174a1f3c643",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512173998055,
          "user_tz": 480,
          "elapsed": 2988,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Fetch the problem\n",
        "ende_problem = problems.problem(\"translate_ende_wmt32k\")\n",
        "\n",
        "# Copy the vocab file locally so we can encode inputs and decode model outputs\n",
        "# All vocabs are stored on GCS\n",
        "vocab_file = os.path.join(gs_data_dir, \"vocab.ende.32768\")\n",
        "!gsutil cp {vocab_file} {data_dir}\n",
        "\n",
        "# Get the encoders from the problem\n",
        "encoders = ende_problem.feature_encoders(data_dir)\n",
        "\n",
        "# Setup helper functions for encoding and decoding\n",
        "def encode(input_str):\n",
        "  \"\"\"Input str to features dict, ready for inference\"\"\"\n",
        "  inputs = encoders[\"inputs\"].encode(input_str) + [1]  # add EOS id\n",
        "  batch_inputs = tf.reshape(inputs, [1, -1, 1])  # Make it 3D.\n",
        "  return {\"inputs\": batch_inputs}\n",
        "\n",
        "def decode(integers):\n",
        "  \"\"\"List of ints to str\"\"\"\n",
        "  integers = list(np.squeeze(integers))\n",
        "  if 1 in integers:\n",
        "    integers = integers[:integers.index(1)]\n",
        "  return encoders[\"inputs\"].decode(np.squeeze(integers))"
      ],
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://tensor2tensor-data/vocab.ende.32768...\n",
            "/ [1 files][316.4 KiB/316.4 KiB]                                                \n",
            "Operation completed over 1 objects/316.4 KiB.                                    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g2aQW7Z6TOEu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# # Generate and view the data\n",
        "# # This cell is commented out because WMT data generation can take hours\n",
        "\n",
        "# ende_problem.generate_data(data_dir, tmp_dir)\n",
        "# example = tfe.Iterator(ende_problem.dataset(Modes.TRAIN, data_dir)).next()\n",
        "# inputs = [int(x) for x in example[\"inputs\"].numpy()] # Cast to ints.\n",
        "# targets = [int(x) for x in example[\"targets\"].numpy()] # Cast to ints.\n",
        "\n",
        "\n",
        "\n",
        "# # Example inputs as int-tensor.\n",
        "# print(\"Inputs, encoded:\")\n",
        "# print(inputs)\n",
        "# print(\"Inputs, decoded:\")\n",
        "# # Example inputs as a sentence.\n",
        "# print(decode(inputs))\n",
        "# # Example targets as int-tensor.\n",
        "# print(\"Targets, encoded:\")\n",
        "# print(targets)\n",
        "# # Example targets as a sentence.\n",
        "# print(\"Targets, decoded:\")\n",
        "# print(decode(targets))"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WkFUEs7ZOA79",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "7283214e-af66-4f16-b203-3b209643484f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174000121,
          "user_tz": 480,
          "elapsed": 321,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# There are many models available in Tensor2Tensor\n",
        "registry.list_models()"
      ],
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resnet50',\n",
              " 'lstm_seq2seq',\n",
              " 'transformer_encoder',\n",
              " 'attention_lm',\n",
              " 'vanilla_gan',\n",
              " 'transformer',\n",
              " 'gene_expression_conv',\n",
              " 'transformer_moe',\n",
              " 'attention_lm_moe',\n",
              " 'transformer_revnet',\n",
              " 'lstm_seq2seq_attention',\n",
              " 'shake_shake',\n",
              " 'transformer_ae',\n",
              " 'diagonal_neural_gpu',\n",
              " 'xception',\n",
              " 'aligned',\n",
              " 'multi_model',\n",
              " 'neural_gpu',\n",
              " 'slice_net',\n",
              " 'byte_net',\n",
              " 'cycle_gan',\n",
              " 'transformer_sketch',\n",
              " 'blue_net']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "9l6hDQbrRUYV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Create hparams and the model\n",
        "model_name = \"transformer\"\n",
        "hparams_set = \"transformer_base\"\n",
        "\n",
        "hparams = trainer_utils.create_hparams(hparams_set, data_dir)\n",
        "trainer_utils.add_problem_hparams(hparams, \"translate_ende_wmt32k\")\n",
        "\n",
        "# NOTE: Only create the model once when restoring from a checkpoint; it's a\n",
        "# Layer and so subsequent instantiations will have different variable scopes\n",
        "# that will not match the checkpoint.\n",
        "translate_model = registry.model(model_name)(hparams, Modes.PREDICT)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FEwNUVlMYOJi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec8569a0-ee0e-4520-c9c6-06f3c7582ecc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174015202,
          "user_tz": 480,
          "elapsed": 12781,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Copy the pretrained checkpoint locally\n",
        "ckpt_name = \"transformer_ende_test\"\n",
        "gs_ckpt = os.path.join(gs_ckpt_dir, ckpt_name)\n",
        "!gsutil -q cp -R {gs_ckpt} {checkpoint_dir}\n",
        "ckpt_path = tf.train.latest_checkpoint(os.path.join(checkpoint_dir, ckpt_name))\n",
        "ckpt_path"
      ],
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'/content/t2t/checkpoints/transformer_ende_test/model.ckpt-350855'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "3O-8E9d6TtuJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 3
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "306d8df1-70c4-43f5-fc15-54ff66ec58ed",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174026517,
          "user_tz": 480,
          "elapsed": 11277,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Restore and translate!\n",
        "\n",
        "def translate(inputs):\n",
        "  encoded_inputs = encode(inputs)\n",
        "  with tfe.restore_variables_on_create(ckpt_path):\n",
        "    model_output = translate_model.infer(encoded_inputs)\n",
        "  return decode(model_output)\n",
        "\n",
        "inputs = \"This is a cat.\"\n",
        "outputs = translate(inputs)\n",
        "\n",
        "print(\"Inputs: %s\" % inputs)\n",
        "print(\"Outputs: %s\" % outputs)"
      ],
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Greedy Decoding\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:487: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "Inputs: This is a cat.\n",
            "Outputs: Das ist eine Katze.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i7BZuO7T5BB4",
        "colab_type": "text"
      },
      "source": [
        "# Train a custom model on MNIST"
      ],
      "cell_type": "markdown"
    },
    {
      "metadata": {
        "id": "-H25oG91YQj3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# Create your own model\n",
        "\n",
        "class MySimpleModel(t2t_model.T2TModel):\n",
        "\n",
        "  def model_fn_body(self, features):\n",
        "    inputs = features[\"inputs\"]\n",
        "    filters = self.hparams.hidden_size\n",
        "    h1 = tf.layers.conv2d(inputs, filters,\n",
        "                          kernel_size=(5, 5), strides=(2, 2))\n",
        "    h2 = tf.layers.conv2d(tf.nn.relu(h1), filters,\n",
        "                          kernel_size=(5, 5), strides=(2, 2))\n",
        "    return tf.layers.conv2d(tf.nn.relu(h2), filters,\n",
        "                            kernel_size=(3, 3))\n",
        "\n",
        "hparams = trainer_utils.create_hparams(\"basic_1\", data_dir)\n",
        "hparams.hidden_size = 64\n",
        "trainer_utils.add_problem_hparams(hparams, \"image_mnist\")\n",
        "model = MySimpleModel(hparams, Modes.TRAIN)"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7GEmpYQ2ZMnB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9535b122-d663-470b-fb03-15541769a8d6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174027233,
          "user_tz": 480,
          "elapsed": 372,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Prepare for the training loop\n",
        "\n",
        "# In Eager mode, opt.minimize must be passed a loss function wrapped with\n",
        "# implicit_value_and_gradients\n",
        "@tfe.implicit_value_and_gradients\n",
        "def loss_fn(features):\n",
        "  _, losses = model(features)\n",
        "  return losses[\"training\"]\n",
        "\n",
        "# Setup the training data\n",
        "BATCH_SIZE = 128\n",
        "mnist_train_dataset = mnist_problem.dataset(Modes.TRAIN, data_dir)\n",
        "mnist_train_dataset = mnist_train_dataset.repeat(None).batch(BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()"
      ],
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-train*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AWVd2I7PYz6H",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 11
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "adfe2262-ca2a-4d74-ef6f-4caaf5531824",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174129153,
          "user_tz": 480,
          "elapsed": 101898,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "# Train\n",
        "\n",
        "NUM_STEPS = 500\n",
        "\n",
        "for count, example in enumerate(tfe.Iterator(mnist_train_dataset)):\n",
        "  example[\"targets\"] = tf.reshape(example[\"targets\"], [BATCH_SIZE, 1, 1, 1])  # Make it 4D.\n",
        "  loss, gv = loss_fn(example)\n",
        "  optimizer.apply_gradients(gv)\n",
        "\n",
        "  if count % 50 == 0:\n",
        "    print(\"Step: %d, Loss: %.3f\" % (count, loss.numpy()))\n",
        "  if count >= NUM_STEPS:\n",
        "    break"
      ],
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensor2tensor/layers/common_layers.py:1671: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n",
            "Step: 0, Loss: 5.357\n",
            "Step: 50, Loss: 0.746\n",
            "Step: 100, Loss: 0.618\n",
            "Step: 150, Loss: 0.502\n",
            "Step: 200, Loss: 0.395\n",
            "Step: 250, Loss: 0.345\n",
            "Step: 300, Loss: 0.338\n",
            "Step: 350, Loss: 0.175\n",
            "Step: 400, Loss: 0.345\n",
            "Step: 450, Loss: 0.373\n",
            "Step: 500, Loss: 0.292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a2cL8UwLaSYG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "source": [
        "# This will eventually be available at\n",
        "# tensor2tensor.metrics.create_eager_metrics\n",
        "def create_eager_metrics(metric_names):\n",
        "  \"\"\"Create metrics accumulators and averager for Eager mode.\n",
        "\n",
        "  Args:\n",
        "    metric_names: list<str> from tensor2tensor.metrics.Metrics\n",
        "\n",
        "  Returns:\n",
        "    (accum_fn(predictions, targets) => None,\n",
        "     result_fn() => dict<str metric_name, float avg_val>\n",
        "  \"\"\"\n",
        "  metric_fns = dict(\n",
        "      [(name, metrics.METRICS_FNS[name]) for name in metric_names])\n",
        "  tfe_metrics = dict()\n",
        "\n",
        "  for name in metric_names:\n",
        "    tfe_metrics[name] = tfe.metrics.Mean(name=name)\n",
        "\n",
        "  def metric_accum(predictions, targets):\n",
        "    for name, metric_fn in metric_fns.items():\n",
        "      val, weight = metric_fn(predictions, targets,\n",
        "                              weights_fn=common_layers.weights_all)\n",
        "      tfe_metrics[name](np.squeeze(val), np.squeeze(weight))\n",
        "\n",
        "  def metric_means():\n",
        "    avgs = {}\n",
        "    for name in metric_names:\n",
        "      avgs[name] = tfe_metrics[name].result().numpy()\n",
        "    return avgs\n",
        "\n",
        "  return metric_accum, metric_means"
      ],
      "cell_type": "code",
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CIFlkiVOd8jO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "95ec4064-d884-4ea8-acdf-ffe83dc0c230",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1512174132643,
          "user_tz": 480,
          "elapsed": 3097,
          "user": {
            "displayName": "Ryan Sepassi",
            "photoUrl": "//lh4.googleusercontent.com/-dcHmhQy1Y2A/AAAAAAAAAAI/AAAAAAAABEw/if_k14yF4KI/s50-c-k-no/photo.jpg",
            "userId": "107877449274830904926"
          }
        }
      },
      "source": [
        "model.set_mode(Modes.EVAL)\n",
        "mnist_eval_dataset = mnist_problem.dataset(Modes.EVAL, data_dir)\n",
        "\n",
        "# Create eval metric accumulators for accuracy (ACC) and accuracy in\n",
        "# top 5 (ACC_TOP5)\n",
        "metrics_accum, metrics_result = create_eager_metrics(\n",
        "    [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5])\n",
        "\n",
        "for count, example in enumerate(tfe.Iterator(mnist_eval_dataset)):\n",
        "  if count >= 200:\n",
        "    break\n",
        "\n",
        "  # Make the inputs and targets 4D\n",
        "  example[\"inputs\"] = tf.reshape(example[\"inputs\"], [1, 28, 28, 3])\n",
        "  example[\"targets\"] = tf.reshape(example[\"targets\"], [1, 1, 1, 1])\n",
        "\n",
        "  # Call the model\n",
        "  predictions, _ = model(example)\n",
        "\n",
        "  # Compute and accumulate metrics\n",
        "  metrics_accum(predictions, example[\"targets\"])\n",
        "\n",
        "# Print out the averaged metric values on the eval data\n",
        "for name, val in metrics_result().items():\n",
        "  print(\"%s: %.2f\" % (name, val))"
      ],
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reading data files from /content/t2t/data/image_mnist-dev*\n",
            "accuracy_top5: 1.00\n",
            "accuracy: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}